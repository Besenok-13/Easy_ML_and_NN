{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка данных\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}\n",
    "dataset[\"class\"] = dataset[\"class\"].apply(lambda x: transform[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\artbo\\AppData\\Local\\Temp\\ipykernel_17132\\991102324.py:60: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  split_info = -np.sum(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Класс, который в последствии добавляется в словарь для удобного выбора\n",
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature=None,\n",
    "        threshold=None,\n",
    "        childs=None,\n",
    "        value=None,\n",
    "    ):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.childs = childs\n",
    "        self.value = value\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "    \n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.tree = None\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "\n",
    "    def calck_unic(self, a: list):\n",
    "        keys = set(a)\n",
    "        return {key: len(a[a == key]) for key in keys}\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.travers_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def entropy(self, y: list):\n",
    "        hist = self.calck_unic(y)\n",
    "        hist = {key: val / len(y) for key, val in hist.items()}\n",
    "        info = -np.sum([p * np.log2(p) for p in hist.values()])\n",
    "\n",
    "        return info\n",
    "\n",
    "    def information_gain(self, X_column: list, y: list):\n",
    "        if len(set(y)) == 1:\n",
    "            return 0\n",
    "\n",
    "        n = len(y)\n",
    "        # info(T)\n",
    "        parent = self.entropy(y)\n",
    "        uitems = self.calck_unic(X_column)\n",
    "        info_x = np.sum(\n",
    "            [val / n * self.entropy(y[X_column == key]) for key, val in uitems.items()]\n",
    "        )\n",
    "        split_info = -np.sum(\n",
    "            val / n * np.log2(val / n) for val in uitems.values() if val > 0\n",
    "        )\n",
    "\n",
    "        if split_info != 0:\n",
    "            return (parent - info_x) / split_info, list(uitems.keys())\n",
    "        else:\n",
    "            return 0, list(uitems.keys())\n",
    "\n",
    "    def most_common(self, y):\n",
    "        labels = self.calck_unic(y)\n",
    "        return max(labels, key=labels.get)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        best_feature = None\n",
    "        best_gain = -1\n",
    "        uitems = []\n",
    "        for i in range(X.shape[1]):\n",
    "            gain, now_uitems = self.information_gain(X[:, i], y)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = i\n",
    "                uitems = now_uitems + []\n",
    "\n",
    "        return best_feature, uitems\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        n_samples = X.shape[0]\n",
    "        n_labels = len(self.calck_unic(y))\n",
    "\n",
    "        if n_samples <= self.min_samples or depth >= self.max_depth or n_labels <= 1:\n",
    "            return Node(value=self.most_common(y))\n",
    "\n",
    "        best_feature, ukeys = self.best_split(X, y)\n",
    "\n",
    "        # В словаре содержатся не словари, а Node По сути, словарь содержит ссылки на объекты, а нужен он для более удобной навигации.\n",
    "        childs = {\n",
    "            key: self.grow_tree(\n",
    "                X[X[:, best_feature] == key],\n",
    "                y[X[:, best_feature] == key],\n",
    "                depth=depth + 1,\n",
    "            )\n",
    "            for key in ukeys\n",
    "        }\n",
    "\n",
    "        return Node(best_feature, childs=childs)\n",
    "\n",
    "    def travers_tree(self, x, tree):\n",
    "        if tree is None:\n",
    "            return None\n",
    "        \n",
    "        elif tree.is_leaf_node():\n",
    "            return tree.value\n",
    "\n",
    "        return self.travers_tree(\n",
    "            x,\n",
    "            tree.childs.get(\n",
    "                x[tree.feature], tree.childs.get(list(tree.childs.keys())[0])  #None\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "# def preload():\n",
    "    # df = pd.read_csv(\"agaricus-lepiota.csv\")\n",
    "    # y = np.array(df.pop(\"poison\"))\n",
    "\n",
    "    # n = df.shape[1]\n",
    "    # to_del = int(n ** 0.5)\n",
    "    # import random\n",
    "\n",
    "    # for i in range(n - to_del - 1):\n",
    "    #     df.pop(random.choice(df.columns))\n",
    "    # x = np.array(df)\n",
    "    # return x, y\n",
    "\n",
    "\n",
    "def acc_pre_rec(otv, y):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(len(otv)):\n",
    "        if otv[i] == \"p\" and y[i] == \"p\":\n",
    "            tp += 1\n",
    "        elif otv[i] == \"p\" and y[i] == \"e\":\n",
    "            fp += 1\n",
    "        elif otv[i] == \"e\" and y[i] == \"p\":\n",
    "            fn += 1\n",
    "        elif otv[i] == \"e\" and y[i] == \"e\":\n",
    "            tn += 1\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    pre = tp / max((tp + fp), 1)\n",
    "    rec = tp / max(tp + fn, 1)\n",
    "    fpr = fp / max(tn + fp, 1)\n",
    "\n",
    "    return acc, pre, rec, fpr\n",
    "\n",
    "# x, y = preload()\n",
    "x = dataset[dataset.columns[:-1]].to_numpy()[:, :2]\n",
    "y = dataset[\"class\"].to_numpy()\n",
    "\n",
    "\n",
    "clf = DecisionTree(max_depth=2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, shuffle=True)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "otv = clf.predict(x_test)\n",
    "\n",
    "print(f1_score(y_test, otv, average=\"micro\"))\n",
    "\n",
    "\n",
    "# acc, pre, rec, fpr = acc_pre_rec(y_test, otv)\n",
    "# print(f\"accuracy: {acc}\")\n",
    "# print(f\"precision: {pre}\")\n",
    "# print(f\"recall: {rec} \")\n",
    "\n",
    "# points = np.array([[0, 0], [fpr, rec], [1, 1]])\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 7))\n",
    "\n",
    "# plt.plot(\n",
    "#     points[:, 0],\n",
    "#     points[:, 1],\n",
    "#     \"o-r\",\n",
    "#     alpha=0.7,\n",
    "#     label=\"first\",\n",
    "#     lw=5,\n",
    "#     mec=\"b\",\n",
    "#     mew=2,\n",
    "#     ms=10,\n",
    "# )\n",
    "# plt.fill_between(points[:, 0],\n",
    "#     points[:, 1], where=None, interpolate=False, step=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.1, num_iter=1000, fit_intercept=True, verbose=False, reg_lambda=0.01):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __softmax(self, z):\n",
    "        exp_scores = np.exp(z)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        self.theta = []\n",
    "        for i in range(len(np.unique(y))):\n",
    "            theta_i = np.zeros(X.shape[1])\n",
    "            y_i = np.array([1 if label == i else 0 for label in y])\n",
    "            print(f\"theta_{i} = \", theta_i)\n",
    "            for j in range(self.num_iter):\n",
    "                z = np.dot(X, theta_i)\n",
    "                h = self.__sigmoid(z)\n",
    "                gradient = np.dot(X.T, (h - y_i)) / y_i.size\n",
    "                gradient += (self.reg_lambda / y_i.size) * theta_i\n",
    "                theta_i -= self.lr * gradient\n",
    "\n",
    "                # log loss\n",
    "                loss = np.sum(-y_i * np.log(h) - (1 - y_i) * np.log(1 - h)) / y_i.size\n",
    "                if self.verbose and j % 1000 == 0:\n",
    "                    print(f\"loss at iteration {j} for theta_{i}: {loss}\")\n",
    "\n",
    "            self.theta.append(theta_i)\n",
    "\n",
    "        self.theta = np.array(self.theta)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        return np.argmax(self.__softmax(np.dot(X, self.theta.T)), axis=1)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        self.predictions = self.predict(X)\n",
    "        return np.mean(self.predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (988575764.py, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[82], line 23\u001b[1;36m\u001b[0m\n\u001b[1;33m    return np.sum(np.argmax(prev_predicts, axis=2, 0), axis=1)\u001b[0m\n\u001b[1;37m                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, y-self.predict(X))\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        prev_predicts = np.zeros((X.shape[0], len(self.trees)))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            prev_predicts[:, i]  = tree.predict(X)\n",
    "        return np.sum(prev_predicts, axis=1)\n",
    "\n",
    "boost = GradientBoosting()\n",
    "\n",
    "boost.fit(X=x_train, y=y_train)\n",
    "otv = boost.predict(x_test)\n",
    "print(f1_score(y_test, otv, average=\"micro\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
