{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
       "0      1          1            1          1        1     1                1   \n",
       "1      0          1            1          2        1     2                1   \n",
       "2      0          2            1          3        1     3                1   \n",
       "3      1          1            2          3        1     1                1   \n",
       "4      0          1            1          4        2     4                1   \n",
       "5      0          1            2          2        1     2                1   \n",
       "6      0          2            1          3        1     2                1   \n",
       "7      0          2            2          3        1     3                1   \n",
       "8      1          1            2          3        1     1                1   \n",
       "9      0          2            1          2        1     2                1   \n",
       "\n",
       "   gill-spacing  gill-size  gill-color  ...  stalk-surface-below-ring  \\\n",
       "0             1          1           1  ...                         1   \n",
       "1             1          2           1  ...                         1   \n",
       "2             1          2           2  ...                         1   \n",
       "3             1          1           2  ...                         1   \n",
       "4             2          2           1  ...                         1   \n",
       "5             1          2           2  ...                         1   \n",
       "6             1          2           3  ...                         1   \n",
       "7             1          2           2  ...                         1   \n",
       "8             1          1           4  ...                         1   \n",
       "9             1          2           3  ...                         1   \n",
       "\n",
       "   stalk-color-above-ring  stalk-color-below-ring  veil-type  veil-color  \\\n",
       "0                       1                       1          1           1   \n",
       "1                       1                       1          1           1   \n",
       "2                       1                       1          1           1   \n",
       "3                       1                       1          1           1   \n",
       "4                       1                       1          1           1   \n",
       "5                       1                       1          1           1   \n",
       "6                       1                       1          1           1   \n",
       "7                       1                       1          1           1   \n",
       "8                       1                       1          1           1   \n",
       "9                       1                       1          1           1   \n",
       "\n",
       "   ring-number  ring-type  spore-print-color  population  habitat  \n",
       "0            1          1                  1           1        1  \n",
       "1            1          1                  2           2        2  \n",
       "2            1          1                  2           2        3  \n",
       "3            1          1                  1           1        1  \n",
       "4            1          2                  2           3        2  \n",
       "5            1          1                  1           2        2  \n",
       "6            1          1                  1           2        3  \n",
       "7            1          1                  2           1        3  \n",
       "8            1          1                  1           4        2  \n",
       "9            1          1                  1           1        3  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "url = \"./mushrooms.csv\"\n",
    "dataset = pd.read_csv(url)\n",
    "dataset.head(10)\n",
    "\n",
    "for column in dataset.columns[1:]:\n",
    "    unics = dataset[column].unique()\n",
    "    transform = {unic:i+1 for i, unic in enumerate(dataset[column].unique())}\n",
    "    dataset[column] = dataset[column].apply(lambda x: transform[x])\n",
    "\n",
    "transform = {\"e\":0, \"p\":1}\n",
    "dataset[\"class\"] = dataset[\"class\"].apply(lambda x: transform[x])\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_neurons, out_neurons, activ_func = \"sigmoid\", lr=0.03):\n",
    "        self.in_neurons = in_neurons\n",
    "        self.out_neurons = out_neurons\n",
    "        self.activ_func = { \"None\": lambda x : x,\n",
    "                           \"sigmoid\" : lambda x: 1 / (1 + np.exp(-x)),\n",
    "                            \"softmax\": self.softmax\n",
    "                            }.get(activ_func, \"sigmoid\")\n",
    "        \n",
    "        self.activ_func_name = activ_func\n",
    "        self.lr = lr\n",
    "        self.W = np.random.random_sample((self.out_neurons, self.in_neurons))-0.5\n",
    "        self.b = np.random.random_sample(out_neurons)+0.001\n",
    "        self.X = None\n",
    "        self.output = None\n",
    "\n",
    "    def softmax(self, y_pred):\n",
    "        #Решаем проблему огромных экспонент\n",
    "        exp_pred = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
    "        return exp_pred / np.sum(exp_pred, axis=1, keepdims=True)\n",
    "\n",
    "    def activ_grad(self, outgrad):\n",
    "        if self.activ_func_name == \"sigmoid\":\n",
    "            return (1-self.output) * self.output\n",
    "        \n",
    "        elif self.activ_func_name ==\"softmax\":\n",
    "            n = self.output.shape[1]\n",
    "            trans_axes = (0, 2, 1)\n",
    "            y_new = np.tile(self.output[:, :, np.newaxis], (1, 1, n))\n",
    "\n",
    "            return np.matmul(\n",
    "                     (np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, \n",
    "                      np.transpose(outgrad[:, np.newaxis], axes = trans_axes)\n",
    "                      ).squeeze()\n",
    "        return 1\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = self.activ_func(np.dot(X, self.W.T) + self.b)\n",
    "        return self.output.copy()\n",
    "    \n",
    "    def backward(self, out_grad):\n",
    "\n",
    "        if self.activ_func_name != \"softmax\":\n",
    "            out_grad = out_grad * self.activ_grad(out_grad)\n",
    "        else:\n",
    "            out_grad = self.activ_grad(out_grad)\n",
    "            \n",
    "        back_grad = np.dot(out_grad, self.W)\n",
    "        self.b -= np.sum(out_grad, axis=0) * self.lr\n",
    "        self.W -= np.dot(out_grad.T, self.X) * self.lr\n",
    "        return back_grad\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_per_layer:list, num_classes:int, active_funcs_per_layer:dict = {0: \"None\"}, lr:float=0.03, diff_funcs = False, mixing = True ):\n",
    "        self.layers = [n_layer for n_layer in n_per_layer] + [num_classes]\n",
    "        self.mixing = mixing\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if diff_funcs == True:\n",
    "            self.active_funcs = []\n",
    "            flag = active_funcs_per_layer.get(0)\n",
    "\n",
    "            for i in range(len(self.layers) - 1):\n",
    "                flag = active_funcs_per_layer.get(i, flag)\n",
    "                self.active_funcs.append(flag)\n",
    "            \n",
    "        else:\n",
    "            self.active_funcs = [active_funcs_per_layer.get(0) for i in range(len(self.layers)-2)] + [\"softmax\"]\n",
    "        \n",
    "        self.layers = [Linear(self.layers[i], self.layers[i+1], activ_func=self.active_funcs[i], lr=lr) for i in range(len(self.layers) - 1)]\n",
    "\n",
    "\n",
    "    def preload_y(self, y_true):\n",
    "        otv = np.zeros((y_true.shape[0], self.num_classes))\n",
    "        otv[np.arange(otv.shape[0]), y_true] += 1\n",
    "        return otv\n",
    "    \n",
    "\n",
    "    def categirical_crossentropy(self, y_pred, y_true):\n",
    "        y_true = self.preload_y(y_true)\n",
    "\n",
    "        cross_ent = - y_true * np.log(y_pred)\n",
    "        grad = y_pred - y_true\n",
    "        return cross_ent, grad\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        now_x = X.copy()\n",
    "        for layer in self.layers:\n",
    "            now_x = layer.forward(now_x)\n",
    "        \n",
    "        return now_x\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, X, y, batch_size):\n",
    "        if batch_size > y.shape[0]:\n",
    "            return -1\n",
    "        \n",
    "        losses = 0 \n",
    "        if self.mixing:\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "        #Данный блок отвечает за прямое распространение и подсчёт итоговой ошибки на батче\n",
    "        #================================================================\n",
    "        for batch_x in range(batch_size, X.shape[0], batch_size):\n",
    "            now_x = X[batch_x - batch_size: batch_x]\n",
    "            y_softmax = self.predict_proba(now_x)\n",
    "\n",
    "            batch_loss, otv_grad = self.categirical_crossentropy(y_softmax, y[batch_x - batch_size: batch_x])\n",
    "            #================================================================\n",
    "\n",
    "\n",
    "            #Данынй блок - изменение параметров весов нейронки\n",
    "            #================================================================\n",
    "            for layer in reversed(self.layers):\n",
    "                # print(\"ACTIVATE\", layer.activ_func)\n",
    "                otv_grad = layer.backward(otv_grad)\n",
    "            #================================================================\n",
    "\n",
    "            losses+=np.sum(batch_loss)\n",
    "        \n",
    "        print(f\"Average_loss for EPOCH is: {losses/y.shape[0]:>7f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, X, y):\n",
    "        y_softmax = self.predict_proba(X)\n",
    "\n",
    "        loss, _ = self.categirical_crossentropy(y_softmax, y)\n",
    "        self.pre_rec_f1(y, np.argmax(y_softmax, axis=1))\n",
    "        print(f\"TEST Average LOSS IS: {np.sum(loss)/y.shape[0]}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def pre_rec_f1(self, y_true, y_pred):\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        for i in np.unique(y_true):\n",
    "\n",
    "            tp = np.sum((y_pred == i) * (y_true == i))\n",
    "            fp = np.sum((y_pred==i) * (y_true != i))\n",
    "            fn = np.sum((y_pred!=i) * (y_true == i))\n",
    "            precision = tp/max(tp+fp, 1)\n",
    "            recall = tp/max(tp+fn, 1)\n",
    "            f1 = 2*precision*recall/max((precision+recall, 0.000001))\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "        \n",
    "        print(f\"Macro precision = {sum(precision_list)/len(precision_list)}\")\n",
    "        print(f\"Macro recall = {sum(recall_list)/len(recall_list)}\")\n",
    "        print(f\"Macro f1 = {sum(f1_list)/len(f1_list)}\")\n",
    "        print(f\"accuracy = {np.sum(y_pred==y_true)/y_true.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 start.\n",
      "Average_loss for EPOCH is: 0.749140\n",
      "Average_loss for EPOCH is: 0.667710\n",
      "Average_loss for EPOCH is: 0.630177\n",
      "Average_loss for EPOCH is: 0.599095\n",
      "Average_loss for EPOCH is: 0.573592\n",
      "Average_loss for EPOCH is: 0.550924\n",
      "Average_loss for EPOCH is: 0.531517\n",
      "Average_loss for EPOCH is: 0.513532\n",
      "Average_loss for EPOCH is: 0.497704\n",
      "Average_loss for EPOCH is: 0.482699\n",
      "Macro precision = 0.8421974522292994\n",
      "Macro recall = 0.842319968930342\n",
      "Macro f1 = 0.8422535894892353\n",
      "accuracy = 0.8424615384615385\n",
      "TEST Average LOSS IS: 0.4778748596446457\n",
      "EPOCH 9 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.468027\n",
      "Average_loss for EPOCH is: 0.455159\n",
      "Average_loss for EPOCH is: 0.441677\n",
      "Average_loss for EPOCH is: 0.430685\n",
      "Average_loss for EPOCH is: 0.418244\n",
      "Average_loss for EPOCH is: 0.408537\n",
      "Average_loss for EPOCH is: 0.399838\n",
      "Average_loss for EPOCH is: 0.389894\n",
      "Average_loss for EPOCH is: 0.380933\n",
      "Average_loss for EPOCH is: 0.372544\n",
      "Macro precision = 0.9011954928823783\n",
      "Macro recall = 0.90036773852941\n",
      "Macro f1 = 0.9006823638831156\n",
      "accuracy = 0.9009230769230769\n",
      "TEST Average LOSS IS: 0.3731088898430385\n",
      "EPOCH 19 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.365776\n",
      "Average_loss for EPOCH is: 0.359424\n",
      "Average_loss for EPOCH is: 0.351528\n",
      "Average_loss for EPOCH is: 0.347114\n",
      "Average_loss for EPOCH is: 0.340234\n",
      "Average_loss for EPOCH is: 0.335555\n",
      "Average_loss for EPOCH is: 0.330748\n",
      "Average_loss for EPOCH is: 0.327347\n",
      "Average_loss for EPOCH is: 0.321372\n",
      "Average_loss for EPOCH is: 0.319353\n",
      "Macro precision = 0.9214042519498\n",
      "Macro recall = 0.9198507503443756\n",
      "Macro f1 = 0.9203711265799945\n",
      "accuracy = 0.9206153846153846\n",
      "TEST Average LOSS IS: 0.31925244915713\n",
      "EPOCH 29 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.315500\n",
      "Average_loss for EPOCH is: 0.312381\n",
      "Average_loss for EPOCH is: 0.307393\n",
      "Average_loss for EPOCH is: 0.306942\n",
      "Average_loss for EPOCH is: 0.303775\n",
      "Average_loss for EPOCH is: 0.301309\n",
      "Average_loss for EPOCH is: 0.298662\n",
      "Average_loss for EPOCH is: 0.295585\n",
      "Average_loss for EPOCH is: 0.294233\n",
      "Average_loss for EPOCH is: 0.291652\n",
      "Macro precision = 0.9328814504966494\n",
      "Macro recall = 0.9308009842770539\n",
      "Macro f1 = 0.9314530732394699\n",
      "accuracy = 0.9316923076923077\n",
      "TEST Average LOSS IS: 0.2926437153464838\n",
      "EPOCH 39 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.289829\n",
      "Average_loss for EPOCH is: 0.288635\n",
      "Average_loss for EPOCH is: 0.286206\n",
      "Average_loss for EPOCH is: 0.287926\n",
      "Average_loss for EPOCH is: 0.284881\n",
      "Average_loss for EPOCH is: 0.282787\n",
      "Average_loss for EPOCH is: 0.281879\n",
      "Average_loss for EPOCH is: 0.280449\n",
      "Average_loss for EPOCH is: 0.277823\n",
      "Average_loss for EPOCH is: 0.278989\n",
      "Macro precision = 0.9437425037309675\n",
      "Macro recall = 0.9411588011481209\n",
      "Macro f1 = 0.9419295140251154\n",
      "accuracy = 0.9421538461538461\n",
      "TEST Average LOSS IS: 0.2781369950551529\n",
      "EPOCH 49 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.278565\n",
      "Average_loss for EPOCH is: 0.276991\n",
      "Average_loss for EPOCH is: 0.276061\n",
      "Average_loss for EPOCH is: 0.274074\n",
      "Average_loss for EPOCH is: 0.273858\n",
      "Average_loss for EPOCH is: 0.274800\n",
      "Average_loss for EPOCH is: 0.272812\n",
      "Average_loss for EPOCH is: 0.271947\n",
      "Average_loss for EPOCH is: 0.272447\n",
      "Average_loss for EPOCH is: 0.269886\n",
      "Macro precision = 0.9489976388078576\n",
      "Macro recall = 0.9459937132489031\n",
      "Macro f1 = 0.9468550349863096\n",
      "accuracy = 0.947076923076923\n",
      "TEST Average LOSS IS: 0.2717020812264198\n",
      "EPOCH 59 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.270819\n",
      "Average_loss for EPOCH is: 0.269755\n",
      "Average_loss for EPOCH is: 0.270111\n",
      "Average_loss for EPOCH is: 0.272196\n",
      "Average_loss for EPOCH is: 0.270304\n",
      "Average_loss for EPOCH is: 0.269838\n",
      "Average_loss for EPOCH is: 0.268031\n",
      "Average_loss for EPOCH is: 0.266124\n",
      "Average_loss for EPOCH is: 0.267248\n",
      "Average_loss for EPOCH is: 0.269438\n",
      "Macro precision = 0.9527204110245986\n",
      "Macro recall = 0.9496915790304082\n",
      "Macro f1 = 0.9505628232430787\n",
      "accuracy = 0.9507692307692308\n",
      "TEST Average LOSS IS: 0.2679932928815472\n",
      "EPOCH 69 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.268119\n",
      "Average_loss for EPOCH is: 0.266057\n",
      "Average_loss for EPOCH is: 0.267184\n",
      "Average_loss for EPOCH is: 0.270571\n",
      "Average_loss for EPOCH is: 0.269653\n",
      "Average_loss for EPOCH is: 0.269680\n",
      "Average_loss for EPOCH is: 0.267053\n",
      "Average_loss for EPOCH is: 0.268526\n",
      "Average_loss for EPOCH is: 0.267297\n",
      "Average_loss for EPOCH is: 0.267617\n",
      "Macro precision = 0.9532115086963704\n",
      "Macro recall = 0.9495482156185714\n",
      "Macro f1 = 0.9505384460853844\n",
      "accuracy = 0.9507692307692308\n",
      "TEST Average LOSS IS: 0.26945980929065094\n",
      "EPOCH 79 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.269446\n",
      "Average_loss for EPOCH is: 0.270357\n",
      "Average_loss for EPOCH is: 0.271800\n",
      "Average_loss for EPOCH is: 0.271222\n",
      "Average_loss for EPOCH is: 0.270676\n",
      "Average_loss for EPOCH is: 0.270713\n",
      "Average_loss for EPOCH is: 0.272099\n",
      "Average_loss for EPOCH is: 0.269932\n",
      "Average_loss for EPOCH is: 0.270566\n",
      "Average_loss for EPOCH is: 0.267737\n",
      "Macro precision = 0.9578288819162777\n",
      "Macro recall = 0.9537907106577422\n",
      "Macro f1 = 0.9548547378644396\n",
      "accuracy = 0.955076923076923\n",
      "TEST Average LOSS IS: 0.2706961159653286\n",
      "EPOCH 89 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.270854\n",
      "Average_loss for EPOCH is: 0.273048\n",
      "Average_loss for EPOCH is: 0.273901\n",
      "Average_loss for EPOCH is: 0.270886\n",
      "Average_loss for EPOCH is: 0.272736\n",
      "Average_loss for EPOCH is: 0.270630\n",
      "Average_loss for EPOCH is: 0.272388\n",
      "Average_loss for EPOCH is: 0.275503\n",
      "Average_loss for EPOCH is: 0.274781\n",
      "Average_loss for EPOCH is: 0.277770\n",
      "Macro precision = 0.9585430751518714\n",
      "Macro recall = 0.9543831277193536\n",
      "Macro f1 = 0.9554692852249371\n",
      "accuracy = 0.9556923076923077\n",
      "TEST Average LOSS IS: 0.27328607521135606\n",
      "EPOCH 99 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset[dataset.columns[1:]].to_numpy()\n",
    "y = dataset[\"class\"].to_numpy()\n",
    "\n",
    "x_train, x_test, y_train1, y_test1 = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = MLP(n_per_layer=(22,10,5), num_classes=2, lr=0.00003)\n",
    "\n",
    "for i in range(100):\n",
    "    if i%100 == 0:\n",
    "        print(f\"EPOCH {i} start.\")\n",
    "\n",
    "    clf.train(x_train, y_train1, batch_size=100)\n",
    "    \n",
    "    if (i + 1) %10 == 0:\n",
    "        clf.test(x_test, y_test1)\n",
    "        print(f\"EPOCH {i} end.\\n ======================\\n\\n\\n\\n======================\")\n",
    "\n",
    "otv = clf.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def AUC_ROC(y_true, y_pred_proba, class_ind=1):\n",
    "    # y_true1, y_pred_proba_1 = list(zip(*list(sorted(zip(y_true, y_pred_proba), key = lambda x : x[1]))))\n",
    "    sorted_indices = np.argsort(y_pred_proba)[::-1]\n",
    "    sorted_y_true = y_true[sorted_indices]\n",
    "    sorted_y_pred = y_pred_proba[sorted_indices]\n",
    "    otv_list_y = []\n",
    "    otv_list_x = []\n",
    "    flag_y = 0\n",
    "    flag_x = 0\n",
    "    for y in sorted_y_true:\n",
    "        if y == 1:\n",
    "            flag_y+=1\n",
    "        else:\n",
    "            flag_x += 1\n",
    "        otv_list_y.append(flag_y)\n",
    "        otv_list_x.append(flag_x)\n",
    "\n",
    "    otv_list_x = np.array(otv_list_x)\n",
    "    otv_list_y = np.array(otv_list_y)\n",
    "    plt.plot(otv_list_x/otv_list_x[-1], otv_list_y/otv_list_y[-1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'AUC - ROC Curve for class: {class_ind}')\n",
    "    plt.show()\n",
    "\n",
    "AUC_ROC(y_test1, otv[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
