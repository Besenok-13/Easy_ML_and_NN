{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal-length  sepal-width  petal-length  petal-width   class\n",
      "0           5.1          3.5           1.4          0.2  Setosa\n",
      "1           4.9          3.0           1.4          0.2  Setosa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
       "0      1          1            1          1        1     1                1   \n",
       "1      0          1            1          2        1     2                1   \n",
       "2      0          2            1          3        1     3                1   \n",
       "3      1          1            2          3        1     1                1   \n",
       "4      0          1            1          4        2     4                1   \n",
       "5      0          1            2          2        1     2                1   \n",
       "6      0          2            1          3        1     2                1   \n",
       "7      0          2            2          3        1     3                1   \n",
       "8      1          1            2          3        1     1                1   \n",
       "9      0          2            1          2        1     2                1   \n",
       "\n",
       "   gill-spacing  gill-size  gill-color  ...  stalk-surface-below-ring  \\\n",
       "0             1          1           1  ...                         1   \n",
       "1             1          2           1  ...                         1   \n",
       "2             1          2           2  ...                         1   \n",
       "3             1          1           2  ...                         1   \n",
       "4             2          2           1  ...                         1   \n",
       "5             1          2           2  ...                         1   \n",
       "6             1          2           3  ...                         1   \n",
       "7             1          2           2  ...                         1   \n",
       "8             1          1           4  ...                         1   \n",
       "9             1          2           3  ...                         1   \n",
       "\n",
       "   stalk-color-above-ring  stalk-color-below-ring  veil-type  veil-color  \\\n",
       "0                       1                       1          1           1   \n",
       "1                       1                       1          1           1   \n",
       "2                       1                       1          1           1   \n",
       "3                       1                       1          1           1   \n",
       "4                       1                       1          1           1   \n",
       "5                       1                       1          1           1   \n",
       "6                       1                       1          1           1   \n",
       "7                       1                       1          1           1   \n",
       "8                       1                       1          1           1   \n",
       "9                       1                       1          1           1   \n",
       "\n",
       "   ring-number  ring-type  spore-print-color  population  habitat  \n",
       "0            1          1                  1           1        1  \n",
       "1            1          1                  2           2        2  \n",
       "2            1          1                  2           2        3  \n",
       "3            1          1                  1           1        1  \n",
       "4            1          2                  2           3        2  \n",
       "5            1          1                  1           2        2  \n",
       "6            1          1                  1           2        3  \n",
       "7            1          1                  2           1        3  \n",
       "8            1          1                  1           4        2  \n",
       "9            1          1                  1           1        3  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка данных\n",
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "url = \"./iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "print(dataset.head(2))\n",
    "transform = {'Setosa':0, 'Versicolor':1, 'Virginica':2}\n",
    "dataset[\"class\"] = dataset[\"class\"].apply(lambda x: transform[x])\n",
    "\n",
    "url = \"./mushrooms.csv\"\n",
    "dataset = pd.read_csv(url)\n",
    "dataset.head(10)\n",
    "\n",
    "for column in dataset.columns[1:]:\n",
    "    unics = dataset[column].unique()\n",
    "    transform = {unic:i+1 for i, unic in enumerate(dataset[column].unique())}\n",
    "    dataset[column] = dataset[column].apply(lambda x: transform[x])\n",
    "\n",
    "transform = {\"e\":0, \"p\":1}\n",
    "dataset[\"class\"] = dataset[\"class\"].apply(lambda x: transform[x])\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_neurons, out_neurons, activ_func = \"sigmoid\", lr=0.03):\n",
    "        self.in_neurons = in_neurons\n",
    "        self.out_neurons = out_neurons\n",
    "        self.activ_func = { \"None\": lambda x : x,\n",
    "                           \"sigmoid\" : lambda x: 1 / (1 + np.exp(-x)),\n",
    "                            \"softmax\": self.softmax\n",
    "                            }.get(activ_func, \"sigmoid\")\n",
    "        \n",
    "        self.activ_func_name = activ_func\n",
    "        self.lr = lr\n",
    "        self.W = np.random.random_sample((self.out_neurons, self.in_neurons))-0.5\n",
    "        self.b = np.random.random_sample(out_neurons)+0.001\n",
    "        self.X = None\n",
    "        self.output = None\n",
    "\n",
    "    def softmax(self, y_pred):\n",
    "        #Решаем проблему огромных экспонент\n",
    "        exp_pred = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
    "        # print(\"EXPPRED:\", exp_pred)\n",
    "        # exp_pred = np.exp(y_pred)\n",
    "        return exp_pred / np.sum(exp_pred, axis=1, keepdims=True)\n",
    "\n",
    "    def activ_grad(self, outgrad):\n",
    "        if self.activ_func_name == \"sigmoid\":\n",
    "            # print(\"OUTPUT: \", self.output)\n",
    "            return (1-self.output) * self.output\n",
    "        # return (1-self.activ_func(self.X)) * self.activ_func(self.X)\n",
    "        elif self.activ_func_name ==\"softmax\":\n",
    "            # print(\"yeh\")\n",
    "            n = self.output.shape[1]\n",
    "            # print(\"OUTPUT SOFTMAX IS:\",self.output)\n",
    "            trans_axes = (0, 2, 1)\n",
    "            y_new = np.tile(self.output[:, :, np.newaxis], (1, 1, n))\n",
    "            # np.matmul((np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, \n",
    "            #           np.transpose(outgrad[:, np.newaxis], axes = trans_axes)).squeeze()\n",
    "            # НЕПРАВИЛЬНЫЙ ГРАДИЕНТ ФУНКЦИИ АКТИВАЦИИ ДЛЯ SOFTMAX!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            return np.matmul(\n",
    "                     (np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, \n",
    "                      np.transpose(outgrad[:, np.newaxis], axes = trans_axes)\n",
    "                      ).squeeze()\n",
    "        return 1\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = self.activ_func(np.dot(X, self.W.T) + self.b)\n",
    "        return self.output.copy()\n",
    "    \n",
    "    def backward(self, out_grad):\n",
    "        # print(\"out_grad\",out_grad)\n",
    "        if self.activ_func_name != \"softmax\":\n",
    "            out_grad = out_grad * self.activ_grad(out_grad)\n",
    "        else:\n",
    "            out_grad = self.activ_grad(out_grad)\n",
    "            \n",
    "        # print(\"OUTgrad after activ\", out_grad)\n",
    "        # print(\"W\", self.W)\n",
    "        # print(\"b\", self.b)\n",
    "        # print(\"X\", self.X)\n",
    "        # print(\"Out_trans\", out_grad.T)\n",
    "        back_grad = np.dot(out_grad, self.W)\n",
    "        self.b -= np.sum(out_grad, axis=0) * self.lr\n",
    "        self.W -= np.dot(out_grad.T, self.X) * self.lr\n",
    "        # print(\"NEW W\", self.W)\n",
    "        return back_grad\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_per_layer:list, num_classes:int, active_funcs_per_layer:dict = {0: \"None\"}, lr:float=0.03, diff_funcs = False, mixing = True ):\n",
    "        self.layers = [n_layer for n_layer in n_per_layer] + [num_classes]\n",
    "        self.mixing = mixing\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if diff_funcs == True:\n",
    "            self.active_funcs = []\n",
    "            flag = active_funcs_per_layer.get(0)\n",
    "\n",
    "            for i in range(len(self.layers) - 1):\n",
    "                flag = active_funcs_per_layer.get(i, flag)\n",
    "                self.active_funcs.append(flag)\n",
    "            \n",
    "        else:\n",
    "            self.active_funcs = [active_funcs_per_layer.get(0) for i in range(len(self.layers)-2)] + [\"softmax\"]\n",
    "        \n",
    "        self.layers = [Linear(self.layers[i], self.layers[i+1], activ_func=self.active_funcs[i], lr=lr) for i in range(len(self.layers) - 1)]\n",
    "\n",
    "\n",
    "    def preload_y(self, y_true):\n",
    "        otv = np.zeros((y_true.shape[0], self.num_classes))\n",
    "        otv[np.arange(otv.shape[0]), y_true] += 1\n",
    "        return otv\n",
    "    \n",
    "\n",
    "    def categirical_crossentropy(self, y_pred, y_true):\n",
    "        # y_pred = self.softmax(y_pred)\n",
    "        y_true = self.preload_y(y_true)\n",
    "\n",
    "        # print(y_true)\n",
    "        # print(y_pred)\n",
    "        cross_ent = - y_true * np.log(y_pred) #np.sum(y_true * np.log(y_pred), axis=-1)# / y_true.shape[0]\n",
    "        grad = y_pred - y_true\n",
    "        # print(f\"{grad=}\")\n",
    "        # print(f\"{cross_ent=}\")\n",
    "        return cross_ent, grad\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        now_x = X.copy()\n",
    "        for layer in self.layers:\n",
    "            now_x = layer.forward(now_x)\n",
    "        \n",
    "        return now_x\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, X, y, batch_size):\n",
    "        if batch_size > y.shape[0]:\n",
    "            return -1\n",
    "        \n",
    "        losses = 0 \n",
    "        if self.mixing:\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "        #Данный блок отвечает за прямое распространение и подсчёт итоговой ошибки на батче\n",
    "        #================================================================\n",
    "        for batch_x in range(batch_size, X.shape[0], batch_size):\n",
    "            # print(\"now\",batch_x)\n",
    "            now_x = X[batch_x - batch_size: batch_x]\n",
    "            # print(\"NOW_X IS\", now_x)\n",
    "            y_softmax = self.predict_proba(now_x)\n",
    "\n",
    "            batch_loss, otv_grad = self.categirical_crossentropy(y_softmax, y[batch_x - batch_size: batch_x])\n",
    "            #================================================================\n",
    "\n",
    "\n",
    "            #Данынй блок - изменение параметров весов нейронки\n",
    "            #================================================================\n",
    "            for layer in reversed(self.layers):\n",
    "                # print(\"ACTIVATE\", layer.activ_func)\n",
    "                otv_grad = layer.backward(otv_grad)\n",
    "            #================================================================\n",
    "\n",
    "            losses+=np.sum(batch_loss)\n",
    "\n",
    "            # if batch_x+1%1 == 0:\n",
    "            # print(f\"Average_loss [{batch_x:>5d}/{y.shape[0]:>5d}] is: {losses/batch_x:>7f}\")\n",
    "        \n",
    "        print(f\"Average_loss for EPOCH is: {losses/y.shape[0]:>7f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, X, y):\n",
    "        y_softmax = self.predict_proba(X)\n",
    "\n",
    "        loss, _ = self.categirical_crossentropy(y_softmax, y)\n",
    "        self.pre_rec_f1(y, np.argmax(y_softmax, axis=1))\n",
    "        print(f\"TEST Average LOSS IS: {np.sum(loss)/y.shape[0]}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def pre_rec_f1(self, y_true, y_pred):\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        for i in np.unique(y_true):\n",
    "\n",
    "            tp = np.sum((y_pred == i) * (y_true == i))\n",
    "            fp = np.sum((y_pred==i) * (y_true != i))\n",
    "            fn = np.sum((y_pred!=i) * (y_true == i))\n",
    "            precision = tp/max(tp+fp, 1)\n",
    "            recall = tp/max(tp+fn, 1)\n",
    "            f1 = 2*precision*recall/max((precision+recall, 0.000001))\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "        \n",
    "        print(f\"Macro precision = {sum(precision_list)/len(precision_list)}\")\n",
    "        print(f\"Macro recall = {sum(recall_list)/len(recall_list)}\")\n",
    "        print(f\"Macro f1 = {sum(f1_list)/len(f1_list)}\")\n",
    "        print(f\"accuracy = {np.sum(y_pred==y_true)/y_true.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 start.\n",
      "Average_loss for EPOCH is: 0.494991\n",
      "Macro precision = 0.863935922748213\n",
      "Macro recall = 0.8662284115081653\n",
      "Macro f1 = 0.8620572568493877\n",
      "accuracy = 0.8621538461538462\n",
      "TEST Average LOSS IS: 0.3371550860925283\n",
      "EPOCH 0 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.248379\n",
      "Macro precision = 0.9387347004096305\n",
      "Macro recall = 0.935071246982418\n",
      "Macro f1 = 0.9365279101043738\n",
      "accuracy = 0.9372307692307692\n",
      "TEST Average LOSS IS: 0.24614603724671388\n",
      "EPOCH 1 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.251705\n",
      "Macro precision = 0.9459346695021593\n",
      "Macro recall = 0.9370939044078825\n",
      "Macro f1 = 0.9399865200491188\n",
      "accuracy = 0.940923076923077\n",
      "TEST Average LOSS IS: 0.2908417270619911\n",
      "EPOCH 2 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.255644\n",
      "Macro precision = 0.9469390104283333\n",
      "Macro recall = 0.9384397994280709\n",
      "Macro f1 = 0.9412550611024281\n",
      "accuracy = 0.9421538461538461\n",
      "TEST Average LOSS IS: 0.3419189738954856\n",
      "EPOCH 3 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.276895\n",
      "Macro precision = 0.9515664887444883\n",
      "Macro recall = 0.9453813827011288\n",
      "Macro f1 = 0.9476139597859425\n",
      "accuracy = 0.9483076923076923\n",
      "TEST Average LOSS IS: 0.3332024415325804\n",
      "EPOCH 4 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.398746\n",
      "Macro precision = 0.9573097602294682\n",
      "Macro recall = 0.9457033598544846\n",
      "Macro f1 = 0.9492572964441749\n",
      "accuracy = 0.9501538461538461\n",
      "TEST Average LOSS IS: 0.370463804769083\n",
      "EPOCH 5 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.388151\n",
      "Macro precision = 0.955773774594274\n",
      "Macro recall = 0.9480037416491944\n",
      "Macro f1 = 0.9506669141487099\n",
      "accuracy = 0.9513846153846154\n",
      "TEST Average LOSS IS: 0.3619084706778991\n",
      "EPOCH 6 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.347315\n",
      "Macro precision = 0.9627492130115425\n",
      "Macro recall = 0.9522207267833109\n",
      "Macro f1 = 0.9555656117310636\n",
      "accuracy = 0.9563076923076923\n",
      "TEST Average LOSS IS: 0.40296191257546915\n",
      "EPOCH 7 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.366817\n",
      "Macro precision = 0.9656810982048574\n",
      "Macro recall = 0.9562584118438762\n",
      "Macro f1 = 0.9593595097208205\n",
      "accuracy = 0.96\n",
      "TEST Average LOSS IS: 0.42388439295298863\n",
      "EPOCH 8 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.398963\n",
      "Macro precision = 0.9471528471528472\n",
      "Macro recall = 0.9439622416934472\n",
      "Macro f1 = 0.9452681020968057\n",
      "accuracy = 0.9458461538461539\n",
      "TEST Average LOSS IS: 0.4408100713861375\n",
      "EPOCH 9 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset[dataset.columns[1:]].to_numpy()\n",
    "y = dataset[\"class\"].to_numpy()\n",
    "\n",
    "x_train, x_test, y_train1, y_test1 = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "cls = MLP(n_per_layer=(22,10,5), num_classes=2, lr=0.003)\n",
    "\n",
    "for i in range(10):\n",
    "    if i%100 == 0:\n",
    "        print(f\"EPOCH {i} start.\")\n",
    "\n",
    "    cls.train(x_train, y_train1, batch_size=100)\n",
    "    \n",
    "    if (i + 1) %1 == 0:\n",
    "        cls.test(x_test, y_test1)\n",
    "        print(f\"EPOCH {i} end.\\n ======================\\n\\n\\n\\n======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.9 0.1]\n",
      "  [0.9 0.1]]\n",
      "\n",
      " [[0.1 0.9]\n",
      "  [0.1 0.9]]\n",
      "\n",
      " [[0.9 0.1]\n",
      "  [0.9 0.1]]]\n",
      "[[[1. 0.]\n",
      "  [0. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.018,  0.018],\n",
       "       [ 0.018, -0.018],\n",
       "       [ 0.162, -0.162]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# def preload_y(y_true):\n",
    "#     otv = np.zeros((y_true.shape[0], 3))\n",
    "#     otv[np.arange(otv.shape[0]), y_true] += 1\n",
    "#     return otv\n",
    "\n",
    "# def categirical_crossentropy( y_pred, y_true):\n",
    "#     y_pred = softmax( y_pred) \n",
    "#     print(y_pred)\n",
    "#     y_true = preload_y(y_true)\n",
    "#     cross_ent = np.sum(y_true * np.log(y_pred), axis=-1) / y_true.shape[0]\n",
    "#     grad = y_pred - y_true\n",
    "#     return cross_ent, grad\n",
    "\n",
    "# # preload_y(np.array([0,0,1,2,0,1,2,1]))\n",
    "\n",
    "# categirical_crossentropy(y_pred=np.array([[0.8,0.1,0.1],\n",
    "#                                           [0.1,0.8,0.1],\n",
    "#                                           [0.1,0.1,0.8],\n",
    "#                                           [0.8,0.1,0.1],\n",
    "#                                           [0.8,0.1,0.1],\n",
    "#                                           [0.8,0.1,0.1]]),\n",
    "                        # y_true=np.array([0,1,2,0,0,1]))\n",
    "# y  = np.array([0,1,2,0,0,1])\n",
    "y_otv = np.array([[1,0,0,0], [0,1,0,0], [0,1,0,0]])\n",
    "y = np.array([[0.9, 0.05, 0.03, 0.02], [0.05, 0.9, 0.03, 0.02], [0.05, 0.03, 0.9, 0.02]])\n",
    "y_otv = np.array([[1,0], [0,1], [0,1]])\n",
    "y = np.array([[0.9, 0.1], [0.1, 0.9], [0.9, 0.1]])\n",
    "# y = np.array([[0.9,0.5,0.5]])\n",
    "# yal = np.diagflat(y)\n",
    "# # np.einsum('ij,ik->ijk', y, y)\n",
    "# # yal-np.einsum('ij,ik->ijk', y, y)\n",
    "# grad_logits = y.copy()\n",
    "# grad_logits[range(X.shape[0]), y_otv] -= 1\n",
    "# grad_logits\n",
    "\n",
    "# class Softmax:\n",
    "#     def backward(out_grad):\n",
    "\n",
    "#         if i == k:\n",
    "#             dx_dy = y[i](1 - y[i])\n",
    "#         else:\n",
    "#             dx_dy = -y[i]*y[k]\n",
    "\n",
    "fard = y - y_otv\n",
    "\n",
    "n = y.shape[1]\n",
    "y_y_matr = np.tile(y, (n, 1))\n",
    "y_new = np.tile(y[:, :, np.newaxis], (1, 1, n))\n",
    "# print(y)\n",
    "# print(np.identity(n)[np.newaxis,:])\n",
    "# print(y_new)\n",
    "trans_axes = (0,2, 1)\n",
    "np.transpose(y_new, axes = trans_axes)\n",
    "print(np.transpose(y_new, axes = trans_axes))\n",
    "print(np.identity(n)[np.newaxis,:])\n",
    "# print(np.transpose(y_new, axes = (0,2, 1)))\n",
    "\n",
    "# print(y_new)\n",
    "# print(np.transpose(fard[:, np.newaxis], axes = trans_axes))\n",
    "np.matmul((np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, np.transpose(fard[:, np.newaxis], axes = trans_axes)).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# print(np.transpose(fard[:, np.newaxis], axes = trans_axes))\n",
    "\n",
    "# np.dot((np.identity(n)  - y_y_matr) * y_y_matr.T,  fard.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
