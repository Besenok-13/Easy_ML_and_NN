{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal-length  sepal-width  petal-length  petal-width   class\n",
      "0           5.1          3.5           1.4          0.2  Setosa\n",
      "1           4.9          3.0           1.4          0.2  Setosa\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cap-shape</th>\n",
       "      <th>cap-surface</th>\n",
       "      <th>cap-color</th>\n",
       "      <th>bruises</th>\n",
       "      <th>odor</th>\n",
       "      <th>gill-attachment</th>\n",
       "      <th>gill-spacing</th>\n",
       "      <th>gill-size</th>\n",
       "      <th>gill-color</th>\n",
       "      <th>...</th>\n",
       "      <th>stalk-surface-below-ring</th>\n",
       "      <th>stalk-color-above-ring</th>\n",
       "      <th>stalk-color-below-ring</th>\n",
       "      <th>veil-type</th>\n",
       "      <th>veil-color</th>\n",
       "      <th>ring-number</th>\n",
       "      <th>ring-type</th>\n",
       "      <th>spore-print-color</th>\n",
       "      <th>population</th>\n",
       "      <th>habitat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
       "0      1          1            1          1        1     1                1   \n",
       "1      0          1            1          2        1     2                1   \n",
       "2      0          2            1          3        1     3                1   \n",
       "3      1          1            2          3        1     1                1   \n",
       "4      0          1            1          4        2     4                1   \n",
       "5      0          1            2          2        1     2                1   \n",
       "6      0          2            1          3        1     2                1   \n",
       "7      0          2            2          3        1     3                1   \n",
       "8      1          1            2          3        1     1                1   \n",
       "9      0          2            1          2        1     2                1   \n",
       "\n",
       "   gill-spacing  gill-size  gill-color  ...  stalk-surface-below-ring  \\\n",
       "0             1          1           1  ...                         1   \n",
       "1             1          2           1  ...                         1   \n",
       "2             1          2           2  ...                         1   \n",
       "3             1          1           2  ...                         1   \n",
       "4             2          2           1  ...                         1   \n",
       "5             1          2           2  ...                         1   \n",
       "6             1          2           3  ...                         1   \n",
       "7             1          2           2  ...                         1   \n",
       "8             1          1           4  ...                         1   \n",
       "9             1          2           3  ...                         1   \n",
       "\n",
       "   stalk-color-above-ring  stalk-color-below-ring  veil-type  veil-color  \\\n",
       "0                       1                       1          1           1   \n",
       "1                       1                       1          1           1   \n",
       "2                       1                       1          1           1   \n",
       "3                       1                       1          1           1   \n",
       "4                       1                       1          1           1   \n",
       "5                       1                       1          1           1   \n",
       "6                       1                       1          1           1   \n",
       "7                       1                       1          1           1   \n",
       "8                       1                       1          1           1   \n",
       "9                       1                       1          1           1   \n",
       "\n",
       "   ring-number  ring-type  spore-print-color  population  habitat  \n",
       "0            1          1                  1           1        1  \n",
       "1            1          1                  2           2        2  \n",
       "2            1          1                  2           2        3  \n",
       "3            1          1                  1           1        1  \n",
       "4            1          2                  2           3        2  \n",
       "5            1          1                  1           2        2  \n",
       "6            1          1                  1           2        3  \n",
       "7            1          1                  2           1        3  \n",
       "8            1          1                  1           4        2  \n",
       "9            1          1                  1           1        3  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Загрузка данных\n",
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "url = \"./iris.csv\"\n",
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "dataset = pd.read_csv(url, names=names)\n",
    "print(dataset.head(2))\n",
    "transform = {'Setosa':0, 'Versicolor':1, 'Virginica':2}\n",
    "dataset[\"class\"] = dataset[\"class\"].apply(lambda x: transform[x])\n",
    "\n",
    "url = \"./mushrooms.csv\"\n",
    "dataset = pd.read_csv(url)\n",
    "dataset.head(10)\n",
    "\n",
    "for column in dataset.columns[1:]:\n",
    "    unics = dataset[column].unique()\n",
    "    transform = {unic:i+1 for i, unic in enumerate(dataset[column].unique())}\n",
    "    dataset[column] = dataset[column].apply(lambda x: transform[x])\n",
    "\n",
    "transform = {\"e\":0, \"p\":1}\n",
    "dataset[\"class\"] = dataset[\"class\"].apply(lambda x: transform[x])\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_neurons, out_neurons, activ_func = \"sigmoid\", lr=0.03):\n",
    "        self.in_neurons = in_neurons\n",
    "        self.out_neurons = out_neurons\n",
    "        self.activ_func = { \"None\": lambda x : x,\n",
    "                           \"sigmoid\" : lambda x: 1 / (1 + np.exp(-x)),\n",
    "                            \"softmax\": self.softmax\n",
    "                            }.get(activ_func, \"sigmoid\")\n",
    "        \n",
    "        self.activ_func_name = activ_func\n",
    "        self.lr = lr\n",
    "        self.W = np.random.random_sample((self.out_neurons, self.in_neurons))-0.5\n",
    "        self.b = np.random.random_sample(out_neurons)+0.001\n",
    "        self.X = None\n",
    "        self.output = None\n",
    "\n",
    "    def softmax(self, y_pred):\n",
    "        #Решаем проблему огромных экспонент\n",
    "        exp_pred = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
    "        # print(\"EXPPRED:\", exp_pred)\n",
    "        # exp_pred = np.exp(y_pred)\n",
    "        return exp_pred / np.sum(exp_pred, axis=1, keepdims=True)\n",
    "\n",
    "    def activ_grad(self, outgrad):\n",
    "        if self.activ_func_name == \"sigmoid\":\n",
    "            # print(\"OUTPUT: \", self.output)\n",
    "            return (1-self.output) * self.output\n",
    "        # return (1-self.activ_func(self.X)) * self.activ_func(self.X)\n",
    "        elif self.activ_func_name ==\"softmax\":\n",
    "            # print(\"yeh\")\n",
    "            n = self.output.shape[1]\n",
    "            # print(\"OUTPUT SOFTMAX IS:\",self.output)\n",
    "            trans_axes = (0, 2, 1)\n",
    "            y_new = np.tile(self.output[:, :, np.newaxis], (1, 1, n))\n",
    "            # np.matmul((np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, \n",
    "            #           np.transpose(outgrad[:, np.newaxis], axes = trans_axes)).squeeze()\n",
    "            # НЕПРАВИЛЬНЫЙ ГРАДИЕНТ ФУНКЦИИ АКТИВАЦИИ ДЛЯ SOFTMAX!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            return np.matmul(\n",
    "                     (np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, \n",
    "                      np.transpose(outgrad[:, np.newaxis], axes = trans_axes)\n",
    "                      ).squeeze()\n",
    "        return 1\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "        self.output = self.activ_func(np.dot(X, self.W.T) + self.b)\n",
    "        return self.output.copy()\n",
    "    \n",
    "    def backward(self, out_grad):\n",
    "        # print(\"out_grad\",out_grad)\n",
    "        if self.activ_func_name != \"softmax\":\n",
    "            out_grad = out_grad * self.activ_grad(out_grad)\n",
    "        else:\n",
    "            out_grad = self.activ_grad(out_grad)\n",
    "            \n",
    "        # print(\"OUTgrad after activ\", out_grad)\n",
    "        # print(\"W\", self.W)\n",
    "        # print(\"b\", self.b)\n",
    "        # print(\"X\", self.X)\n",
    "        # print(\"Out_trans\", out_grad.T)\n",
    "        back_grad = np.dot(out_grad, self.W)\n",
    "        self.b -= np.sum(out_grad, axis=0) * self.lr\n",
    "        self.W -= np.dot(out_grad.T, self.X) * self.lr\n",
    "        # print(\"NEW W\", self.W)\n",
    "        return back_grad\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_per_layer:list, num_classes:int, active_funcs_per_layer:dict = {0: \"None\"}, lr:float=0.03, diff_funcs = False, mixing = True ):\n",
    "        self.layers = [n_layer for n_layer in n_per_layer] + [num_classes]\n",
    "        self.mixing = mixing\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if diff_funcs == True:\n",
    "            self.active_funcs = []\n",
    "            flag = active_funcs_per_layer.get(0)\n",
    "\n",
    "            for i in range(len(self.layers) - 1):\n",
    "                flag = active_funcs_per_layer.get(i, flag)\n",
    "                self.active_funcs.append(flag)\n",
    "            \n",
    "        else:\n",
    "            self.active_funcs = [active_funcs_per_layer.get(0) for i in range(len(self.layers)-2)] + [\"softmax\"]\n",
    "        \n",
    "        self.layers = [Linear(self.layers[i], self.layers[i+1], activ_func=self.active_funcs[i], lr=lr) for i in range(len(self.layers) - 1)]\n",
    "\n",
    "\n",
    "    def preload_y(self, y_true):\n",
    "        otv = np.zeros((y_true.shape[0], self.num_classes))\n",
    "        otv[np.arange(otv.shape[0]), y_true] += 1\n",
    "        return otv\n",
    "    \n",
    "\n",
    "    def categirical_crossentropy(self, y_pred, y_true):\n",
    "        # y_pred = self.softmax(y_pred)\n",
    "        y_true = self.preload_y(y_true)\n",
    "\n",
    "        # print(y_true)\n",
    "        # print(y_pred)\n",
    "        cross_ent = - y_true * np.log(y_pred) #np.sum(y_true * np.log(y_pred), axis=-1)# / y_true.shape[0]\n",
    "        grad = y_pred - y_true\n",
    "        # print(f\"{grad=}\")\n",
    "        # print(f\"{cross_ent=}\")\n",
    "        return cross_ent, grad\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        now_x = X.copy()\n",
    "        for layer in self.layers:\n",
    "            now_x = layer.forward(now_x)\n",
    "        \n",
    "        return now_x\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, X, y, batch_size):\n",
    "        if batch_size > y.shape[0]:\n",
    "            return -1\n",
    "        \n",
    "        losses = 0 \n",
    "        if self.mixing:\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "        #Данный блок отвечает за прямое распространение и подсчёт итоговой ошибки на батче\n",
    "        #================================================================\n",
    "        for batch_x in range(batch_size, X.shape[0], batch_size):\n",
    "            # print(\"now\",batch_x)\n",
    "            now_x = X[batch_x - batch_size: batch_x]\n",
    "            # print(\"NOW_X IS\", now_x)\n",
    "            y_softmax = self.predict_proba(now_x)\n",
    "\n",
    "            batch_loss, otv_grad = self.categirical_crossentropy(y_softmax, y[batch_x - batch_size: batch_x])\n",
    "            #================================================================\n",
    "\n",
    "\n",
    "            #Данынй блок - изменение параметров весов нейронки\n",
    "            #================================================================\n",
    "            for layer in reversed(self.layers):\n",
    "                # print(\"ACTIVATE\", layer.activ_func)\n",
    "                otv_grad = layer.backward(otv_grad)\n",
    "            #================================================================\n",
    "\n",
    "            losses+=np.sum(batch_loss)\n",
    "\n",
    "            # if batch_x+1%1 == 0:\n",
    "            # print(f\"Average_loss [{batch_x:>5d}/{y.shape[0]:>5d}] is: {losses/batch_x:>7f}\")\n",
    "        \n",
    "        print(f\"Average_loss for EPOCH is: {losses/y.shape[0]:>7f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def test(self, X, y):\n",
    "        y_softmax = self.predict_proba(X)\n",
    "\n",
    "        loss, _ = self.categirical_crossentropy(y_softmax, y)\n",
    "        self.pre_rec_f1(y, np.argmax(y_softmax, axis=1))\n",
    "        print(f\"TEST Average LOSS IS: {np.sum(loss)/y.shape[0]}\")\n",
    "        \n",
    "\n",
    "\n",
    "    def pre_rec_f1(self, y_true, y_pred):\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        f1_list = []\n",
    "        for i in np.unique(y_true):\n",
    "\n",
    "            tp = np.sum((y_pred == i) * (y_true == i))\n",
    "            fp = np.sum((y_pred==i) * (y_true != i))\n",
    "            fn = np.sum((y_pred!=i) * (y_true == i))\n",
    "            precision = tp/max(tp+fp, 1)\n",
    "            recall = tp/max(tp+fn, 1)\n",
    "            f1 = 2*precision*recall/max((precision+recall, 0.000001))\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            f1_list.append(f1)\n",
    "        \n",
    "        print(f\"Macro precision = {sum(precision_list)/len(precision_list)}\")\n",
    "        print(f\"Macro recall = {sum(recall_list)/len(recall_list)}\")\n",
    "        print(f\"Macro f1 = {sum(f1_list)/len(f1_list)}\")\n",
    "        print(f\"accuracy = {np.sum(y_pred==y_true)/y_true.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 start.\n",
      "Average_loss for EPOCH is: 0.997461\n",
      "Macro precision = 0.5307109151388567\n",
      "Macro recall = 0.5300689237294555\n",
      "Macro f1 = 0.5274796376646114\n",
      "accuracy = 0.5298461538461539\n",
      "TEST Average LOSS IS: 0.7295866690282514\n",
      "EPOCH 0 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.719905\n",
      "Macro precision = 0.6214116659031913\n",
      "Macro recall = 0.6204461107324093\n",
      "Macro f1 = 0.6196018173367859\n",
      "accuracy = 0.6203076923076923\n",
      "TEST Average LOSS IS: 0.6838465060972654\n",
      "EPOCH 1 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.677420\n",
      "Macro precision = 0.6880010179412139\n",
      "Macro recall = 0.6880027266530334\n",
      "Macro f1 = 0.6879995273838995\n",
      "accuracy = 0.688\n",
      "TEST Average LOSS IS: 0.6472401092409359\n",
      "EPOCH 2 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.644227\n",
      "Macro precision = 0.7337974334249101\n",
      "Macro recall = 0.7335870635461637\n",
      "Macro f1 = 0.7334896128653734\n",
      "accuracy = 0.7335384615384616\n",
      "TEST Average LOSS IS: 0.6161238713256971\n",
      "EPOCH 3 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.614234\n",
      "Macro precision = 0.7651549615287798\n",
      "Macro recall = 0.7648754071044459\n",
      "Macro f1 = 0.7648481845484327\n",
      "accuracy = 0.7649230769230769\n",
      "TEST Average LOSS IS: 0.590476687083838\n",
      "EPOCH 4 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.589945\n",
      "Macro precision = 0.7859252087802868\n",
      "Macro recall = 0.7858706354616375\n",
      "Macro f1 = 0.7858395845704521\n",
      "accuracy = 0.7858461538461539\n",
      "TEST Average LOSS IS: 0.5676247675623879\n",
      "EPOCH 5 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.568633\n",
      "Macro precision = 0.7996226936904903\n",
      "Macro recall = 0.7986859047186246\n",
      "Macro f1 = 0.7985934994263397\n",
      "accuracy = 0.7987692307692308\n",
      "TEST Average LOSS IS: 0.5489942739705529\n",
      "EPOCH 6 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.551167\n",
      "Macro precision = 0.8123324031233241\n",
      "Macro recall = 0.8122926607589185\n",
      "Macro f1 = 0.812297456410404\n",
      "accuracy = 0.8123076923076923\n",
      "TEST Average LOSS IS: 0.5313817132108037\n",
      "EPOCH 7 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.534334\n",
      "Macro precision = 0.821670197964881\n",
      "Macro recall = 0.8215064758009543\n",
      "Macro f1 = 0.821508652427255\n",
      "accuracy = 0.8215384615384616\n",
      "TEST Average LOSS IS: 0.5163298090957875\n",
      "EPOCH 8 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.519164\n",
      "Macro precision = 0.8269230769230769\n",
      "Macro recall = 0.8264030902067712\n",
      "Macro f1 = 0.8263809956808366\n",
      "accuracy = 0.8264615384615385\n",
      "TEST Average LOSS IS: 0.5028613155610955\n",
      "EPOCH 9 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.505718\n",
      "Macro precision = 0.835427284579827\n",
      "Macro recall = 0.8343785503294706\n",
      "Macro f1 = 0.8343169765923101\n",
      "accuracy = 0.8344615384615385\n",
      "TEST Average LOSS IS: 0.49060685926040204\n",
      "EPOCH 10 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.493252\n",
      "Macro precision = 0.8406920570722392\n",
      "Macro recall = 0.8399303188669243\n",
      "Macro f1 = 0.839898080444299\n",
      "accuracy = 0.84\n",
      "TEST Average LOSS IS: 0.4791004275672042\n",
      "EPOCH 11 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.482281\n",
      "Macro precision = 0.8432330129287655\n",
      "Macro recall = 0.8417480875558585\n",
      "Macro f1 = 0.841658107389263\n",
      "accuracy = 0.8418461538461538\n",
      "TEST Average LOSS IS: 0.4686929959275451\n",
      "EPOCH 12 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.471384\n",
      "Macro precision = 0.848405267564893\n",
      "Macro recall = 0.8466636370521852\n",
      "Macro f1 = 0.8465600439889648\n",
      "accuracy = 0.8467692307692307\n",
      "TEST Average LOSS IS: 0.4586270519251731\n",
      "EPOCH 13 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.461059\n",
      "Macro precision = 0.8544719332314679\n",
      "Macro recall = 0.8528213284859502\n",
      "Macro f1 = 0.8527354706265327\n",
      "accuracy = 0.8529230769230769\n",
      "TEST Average LOSS IS: 0.44877281824924853\n",
      "EPOCH 14 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.451837\n",
      "Macro precision = 0.8580665577351991\n",
      "Macro recall = 0.85651745815345\n",
      "Macro f1 = 0.856444898917114\n",
      "accuracy = 0.8566153846153847\n",
      "TEST Average LOSS IS: 0.4391844301595346\n",
      "EPOCH 15 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.442646\n",
      "Macro precision = 0.8610863108795073\n",
      "Macro recall = 0.8602363099295615\n",
      "Macro f1 = 0.8602143121596779\n",
      "accuracy = 0.8603076923076923\n",
      "TEST Average LOSS IS: 0.42941167637742916\n",
      "EPOCH 16 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.429707\n",
      "Macro precision = 0.8622390891840608\n",
      "Macro recall = 0.861470877830796\n",
      "Macro f1 = 0.8614545144513599\n",
      "accuracy = 0.8615384615384616\n",
      "TEST Average LOSS IS: 0.42007849407251463\n",
      "EPOCH 17 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.421285\n",
      "Macro precision = 0.864050186392724\n",
      "Macro recall = 0.863318942664546\n",
      "Macro f1 = 0.8633058795712485\n",
      "accuracy = 0.8633846153846154\n",
      "TEST Average LOSS IS: 0.4106185676392305\n",
      "EPOCH 18 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.411343\n",
      "Macro precision = 0.8666805362360216\n",
      "Macro recall = 0.8657729303946073\n",
      "Macro f1 = 0.8657521517552077\n",
      "accuracy = 0.8658461538461538\n",
      "TEST Average LOSS IS: 0.40104493542199116\n",
      "EPOCH 19 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.399140\n",
      "Macro precision = 0.8657252193789001\n",
      "Macro recall = 0.8645307884571688\n",
      "Macro f1 = 0.8644921735227942\n",
      "accuracy = 0.8646153846153846\n",
      "TEST Average LOSS IS: 0.391692070402785\n",
      "EPOCH 20 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.388778\n",
      "Macro precision = 0.8740673706294131\n",
      "Macro recall = 0.8738089828069378\n",
      "Macro f1 = 0.8738186298814908\n",
      "accuracy = 0.8738461538461538\n",
      "TEST Average LOSS IS: 0.38157469330813576\n",
      "EPOCH 21 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.379237\n",
      "Macro precision = 0.8742836947849644\n",
      "Macro recall = 0.87379383473453\n",
      "Macro f1 = 0.8737972140486384\n",
      "accuracy = 0.8738461538461538\n",
      "TEST Average LOSS IS: 0.37113878664741556\n",
      "EPOCH 22 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.368419\n",
      "Macro precision = 0.8765782196590681\n",
      "Macro recall = 0.8762667575551011\n",
      "Macro f1 = 0.8762760189685482\n",
      "accuracy = 0.8763076923076923\n",
      "TEST Average LOSS IS: 0.36050227308027794\n",
      "EPOCH 23 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.356175\n",
      "Macro precision = 0.8758064516129032\n",
      "Macro recall = 0.8750094675452549\n",
      "Macro f1 = 0.8750011841494492\n",
      "accuracy = 0.8750769230769231\n",
      "TEST Average LOSS IS: 0.3508367670596056\n",
      "EPOCH 24 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.345407\n",
      "Macro precision = 0.8845599939361783\n",
      "Macro recall = 0.8842687268045142\n",
      "Macro f1 = 0.8842803030303031\n",
      "accuracy = 0.8843076923076924\n",
      "TEST Average LOSS IS: 0.3397828096973675\n",
      "EPOCH 25 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.333824\n",
      "Macro precision = 0.8909546953548555\n",
      "Macro recall = 0.8904074831477694\n",
      "Macro f1 = 0.8904163459032193\n",
      "accuracy = 0.8904615384615384\n",
      "TEST Average LOSS IS: 0.33052656702557504\n",
      "EPOCH 26 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.323946\n",
      "Macro precision = 0.892258382642998\n",
      "Macro recall = 0.8916344770128001\n",
      "Macro f1 = 0.8916420398575433\n",
      "accuracy = 0.8916923076923077\n",
      "TEST Average LOSS IS: 0.32261074545414514\n",
      "EPOCH 27 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.315678\n",
      "Macro precision = 0.8987602427275875\n",
      "Macro recall = 0.8977732333560555\n",
      "Macro f1 = 0.8977745742723142\n",
      "accuracy = 0.8978461538461538\n",
      "TEST Average LOSS IS: 0.316353060889747\n",
      "EPOCH 28 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.310420\n",
      "Macro precision = 0.9032272765941932\n",
      "Macro recall = 0.9020752859198666\n",
      "Macro f1 = 0.9020753766615918\n",
      "accuracy = 0.9021538461538462\n",
      "TEST Average LOSS IS: 0.31058432026513955\n",
      "EPOCH 29 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.303883\n",
      "Macro precision = 0.907941837513648\n",
      "Macro recall = 0.9063697644474741\n",
      "Macro f1 = 0.9063619297176149\n",
      "accuracy = 0.9064615384615384\n",
      "TEST Average LOSS IS: 0.30533002501281764\n",
      "EPOCH 30 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.298702\n",
      "Macro precision = 0.9121798170948969\n",
      "Macro recall = 0.9100469590244642\n",
      "Macro f1 = 0.910027062039422\n",
      "accuracy = 0.9101538461538462\n",
      "TEST Average LOSS IS: 0.3009876124217352\n",
      "EPOCH 31 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.294222\n",
      "Macro precision = 0.9159115166959098\n",
      "Macro recall = 0.9143717336968871\n",
      "Macro f1 = 0.9143738572914049\n",
      "accuracy = 0.9144615384615384\n",
      "TEST Average LOSS IS: 0.29583881287782277\n",
      "EPOCH 32 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.290079\n",
      "Macro precision = 0.9193828770099957\n",
      "Macro recall = 0.9180716503824888\n",
      "Macro f1 = 0.9180823713263093\n",
      "accuracy = 0.9181538461538462\n",
      "TEST Average LOSS IS: 0.29135231798151884\n",
      "EPOCH 33 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.286070\n",
      "Macro precision = 0.9182230685395243\n",
      "Macro recall = 0.9162046504582292\n",
      "Macro f1 = 0.9161972194161563\n",
      "accuracy = 0.9163076923076923\n",
      "TEST Average LOSS IS: 0.2882470840787056\n",
      "EPOCH 34 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.280688\n",
      "Macro precision = 0.9216610891390326\n",
      "Macro recall = 0.9199045671438308\n",
      "Macro f1 = 0.9199082499241735\n",
      "accuracy = 0.92\n",
      "TEST Average LOSS IS: 0.28436326989754057\n",
      "EPOCH 35 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.278048\n",
      "Macro precision = 0.9200084921683337\n",
      "Macro recall = 0.9180527152919791\n",
      "Macro f1 = 0.9180494459971918\n",
      "accuracy = 0.9181538461538462\n",
      "TEST Average LOSS IS: 0.28129424388862384\n",
      "EPOCH 36 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.275659\n",
      "Macro precision = 0.921245031016199\n",
      "Macro recall = 0.9192834961751117\n",
      "Macro f1 = 0.9192817851551287\n",
      "accuracy = 0.9193846153846154\n",
      "TEST Average LOSS IS: 0.2783546207628754\n",
      "EPOCH 37 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.271617\n",
      "Macro precision = 0.9233144621718992\n",
      "Macro recall = 0.9211239869726577\n",
      "Macro f1 = 0.9211196160345618\n",
      "accuracy = 0.9212307692307692\n",
      "TEST Average LOSS IS: 0.2755211362229011\n",
      "EPOCH 38 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.268930\n",
      "Macro precision = 0.9244068168118801\n",
      "Macro recall = 0.9223585548738923\n",
      "Macro f1 = 0.922359188576733\n",
      "accuracy = 0.9224615384615384\n",
      "TEST Average LOSS IS: 0.27238612859675976\n",
      "EPOCH 39 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.265363\n",
      "Macro precision = 0.9261911864076606\n",
      "Macro recall = 0.9242066197076422\n",
      "Macro f1 = 0.9242111417868766\n",
      "accuracy = 0.9243076923076923\n",
      "TEST Average LOSS IS: 0.26961143754359546\n",
      "EPOCH 40 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.263993\n",
      "Macro precision = 0.9268803161208224\n",
      "Macro recall = 0.9248201166401575\n",
      "Macro f1 = 0.9248239762409637\n",
      "accuracy = 0.924923076923077\n",
      "TEST Average LOSS IS: 0.2672042058531854\n",
      "EPOCH 41 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.260786\n",
      "Macro precision = 0.9306125550995592\n",
      "Macro recall = 0.9291448913125805\n",
      "Macro f1 = 0.9291637052831082\n",
      "accuracy = 0.9292307692307692\n",
      "TEST Average LOSS IS: 0.2638135471855967\n",
      "EPOCH 42 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.257468\n",
      "Macro precision = 0.9285258955657589\n",
      "Macro recall = 0.9266719684920094\n",
      "Macro f1 = 0.9266821586744057\n",
      "accuracy = 0.9267692307692308\n",
      "TEST Average LOSS IS: 0.2619861209273089\n",
      "EPOCH 43 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.255935\n",
      "Macro precision = 0.9317292876614911\n",
      "Macro recall = 0.930379459213815\n",
      "Macro f1 = 0.9304008117283682\n",
      "accuracy = 0.9304615384615385\n",
      "TEST Average LOSS IS: 0.25895974992600673\n",
      "EPOCH 44 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.253006\n",
      "Macro precision = 0.9318475452196382\n",
      "Macro recall = 0.930375672195713\n",
      "Macro f1 = 0.9303956408434021\n",
      "accuracy = 0.9304615384615385\n",
      "TEST Average LOSS IS: 0.2573137381715365\n",
      "EPOCH 45 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.251572\n",
      "Macro precision = 0.9319711238760175\n",
      "Macro recall = 0.9303718851776112\n",
      "Macro f1 = 0.9303902580858183\n",
      "accuracy = 0.9304615384615385\n",
      "TEST Average LOSS IS: 0.25552548528528835\n",
      "EPOCH 46 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.248825\n",
      "Macro precision = 0.9314171403545588\n",
      "Macro recall = 0.9297546012269939\n",
      "Macro f1 = 0.929771447288211\n",
      "accuracy = 0.9298461538461539\n",
      "TEST Average LOSS IS: 0.2538660862349829\n",
      "EPOCH 47 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.247359\n",
      "Macro precision = 0.9319711238760175\n",
      "Macro recall = 0.9303718851776112\n",
      "Macro f1 = 0.9303902580858183\n",
      "accuracy = 0.9304615384615385\n",
      "TEST Average LOSS IS: 0.2513863857295366\n",
      "EPOCH 48 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.243817\n",
      "Macro precision = 0.9326526826093435\n",
      "Macro recall = 0.9309853821101265\n",
      "Macro f1 = 0.9310035271603478\n",
      "accuracy = 0.931076923076923\n",
      "TEST Average LOSS IS: 0.2500036210017306\n",
      "EPOCH 49 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.243393\n",
      "Macro precision = 0.9353942415084923\n",
      "Macro recall = 0.9334393698401878\n",
      "Macro f1 = 0.9334565871532917\n",
      "accuracy = 0.9335384615384615\n",
      "TEST Average LOSS IS: 0.24833914753700168\n",
      "EPOCH 50 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.240443\n",
      "Macro precision = 0.9366305733840952\n",
      "Macro recall = 0.9346701507233204\n",
      "Macro f1 = 0.9346888725763789\n",
      "accuracy = 0.9347692307692308\n",
      "TEST Average LOSS IS: 0.2467273746080747\n",
      "EPOCH 51 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.239821\n",
      "Macro precision = 0.9373200360384492\n",
      "Macro recall = 0.9352836476558357\n",
      "Macro f1 = 0.9353021942083093\n",
      "accuracy = 0.9353846153846154\n",
      "TEST Average LOSS IS: 0.2451003184037485\n",
      "EPOCH 52 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.238730\n",
      "Macro precision = 0.938964101003955\n",
      "Macro recall = 0.9371354995076877\n",
      "Macro f1 = 0.9371587807097361\n",
      "accuracy = 0.9372307692307692\n",
      "TEST Average LOSS IS: 0.24334541421146513\n",
      "EPOCH 53 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.236623\n",
      "Macro precision = 0.9393832502668366\n",
      "Macro recall = 0.9377565704764068\n",
      "Macro f1 = 0.9377824430678554\n",
      "accuracy = 0.9378461538461539\n",
      "TEST Average LOSS IS: 0.24171614132680316\n",
      "EPOCH 54 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.235067\n",
      "Macro precision = 0.9408871829044537\n",
      "Macro recall = 0.9389797773233356\n",
      "Macro f1 = 0.9390044849476149\n",
      "accuracy = 0.939076923076923\n",
      "TEST Average LOSS IS: 0.24073140308686597\n",
      "EPOCH 55 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.233666\n",
      "Macro precision = 0.9402000304228779\n",
      "Macro recall = 0.9383662803908203\n",
      "Macro f1 = 0.9383909614801333\n",
      "accuracy = 0.9384615384615385\n",
      "TEST Average LOSS IS: 0.23926159454066925\n",
      "EPOCH 56 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.232542\n",
      "Macro precision = 0.941575900886507\n",
      "Macro recall = 0.939593274255851\n",
      "Macro f1 = 0.9396180142687278\n",
      "accuracy = 0.9396923076923077\n",
      "TEST Average LOSS IS: 0.23821108763610102\n",
      "EPOCH 57 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.229766\n",
      "Macro precision = 0.9431093867532365\n",
      "Macro recall = 0.9408164811027797\n",
      "Macro f1 = 0.9408397120259213\n",
      "accuracy = 0.940923076923077\n",
      "TEST Average LOSS IS: 0.23751647100473433\n",
      "EPOCH 58 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.230773\n",
      "Macro precision = 0.94281223276211\n",
      "Macro recall = 0.9408240551389836\n",
      "Macro f1 = 0.9408502996918149\n",
      "accuracy = 0.940923076923077\n",
      "TEST Average LOSS IS: 0.2356232836624857\n",
      "EPOCH 59 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.228853\n",
      "Macro precision = 0.9429580616289477\n",
      "Macro recall = 0.9408202681208817\n",
      "Macro f1 = 0.9408450960584632\n",
      "accuracy = 0.940923076923077\n",
      "TEST Average LOSS IS: 0.23506936789843996\n",
      "EPOCH 60 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.227112\n",
      "Macro precision = 0.9429580616289477\n",
      "Macro recall = 0.9408202681208817\n",
      "Macro f1 = 0.9408450960584632\n",
      "accuracy = 0.940923076923077\n",
      "TEST Average LOSS IS: 0.23430385325654124\n",
      "EPOCH 61 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.227061\n",
      "Macro precision = 0.9424145543122479\n",
      "Macro recall = 0.9402029841702644\n",
      "Macro f1 = 0.9402262018752192\n",
      "accuracy = 0.9403076923076923\n",
      "TEST Average LOSS IS: 0.23358849057352066\n",
      "EPOCH 62 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.225049\n",
      "Macro precision = 0.9436515186115552\n",
      "Macro recall = 0.941433765053397\n",
      "Macro f1 = 0.9414586513210911\n",
      "accuracy = 0.9415384615384615\n",
      "TEST Average LOSS IS: 0.23255272775389998\n",
      "EPOCH 63 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.224161\n",
      "Macro precision = 0.9435027302777761\n",
      "Macro recall = 0.9414375520714989\n",
      "Macro f1 = 0.9414638899979942\n",
      "accuracy = 0.9415384615384615\n",
      "TEST Average LOSS IS: 0.23055977755871393\n",
      "EPOCH 64 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.224607\n",
      "Macro precision = 0.94514374809857\n",
      "Macro recall = 0.9432894039233508\n",
      "Macro f1 = 0.9433196845617228\n",
      "accuracy = 0.9433846153846154\n",
      "TEST Average LOSS IS: 0.22938520491589567\n",
      "EPOCH 65 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.223442\n",
      "Macro precision = 0.94514374809857\n",
      "Macro recall = 0.9432894039233508\n",
      "Macro f1 = 0.9433196845617228\n",
      "accuracy = 0.9433846153846154\n",
      "TEST Average LOSS IS: 0.22857336250458313\n",
      "EPOCH 66 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.223514\n",
      "Macro precision = 0.9445955691060621\n",
      "Macro recall = 0.9426721199727335\n",
      "Macro f1 = 0.9427011828295777\n",
      "accuracy = 0.9427692307692308\n",
      "TEST Average LOSS IS: 0.22805904166705246\n",
      "EPOCH 67 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.221099\n",
      "Macro precision = 0.9445955691060621\n",
      "Macro recall = 0.9426721199727335\n",
      "Macro f1 = 0.9427011828295777\n",
      "accuracy = 0.9427692307692308\n",
      "TEST Average LOSS IS: 0.22697691286395405\n",
      "EPOCH 68 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.220206\n",
      "Macro precision = 0.9440485646377129\n",
      "Macro recall = 0.9420548360221161\n",
      "Macro f1 = 0.9420825851149022\n",
      "accuracy = 0.9421538461538461\n",
      "TEST Average LOSS IS: 0.22673133646095994\n",
      "EPOCH 69 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.220059\n",
      "Macro precision = 0.9440485646377129\n",
      "Macro recall = 0.9420548360221161\n",
      "Macro f1 = 0.9420825851149022\n",
      "accuracy = 0.9421538461538461\n",
      "TEST Average LOSS IS: 0.2262347973995996\n",
      "EPOCH 70 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.218443\n",
      "Macro precision = 0.9445955691060621\n",
      "Macro recall = 0.9426721199727335\n",
      "Macro f1 = 0.9427011828295777\n",
      "accuracy = 0.9427692307692308\n",
      "TEST Average LOSS IS: 0.22486790848570554\n",
      "EPOCH 71 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.217694\n",
      "Macro precision = 0.94514374809857\n",
      "Macro recall = 0.9432894039233508\n",
      "Macro f1 = 0.9433196845617228\n",
      "accuracy = 0.9433846153846154\n",
      "TEST Average LOSS IS: 0.22418472165354336\n",
      "EPOCH 72 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.216924\n",
      "Macro precision = 0.94514374809857\n",
      "Macro recall = 0.9432894039233508\n",
      "Macro f1 = 0.9433196845617228\n",
      "accuracy = 0.9433846153846154\n",
      "TEST Average LOSS IS: 0.2235775859267168\n",
      "EPOCH 73 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.215630\n",
      "Macro precision = 0.9445955691060621\n",
      "Macro recall = 0.9426721199727335\n",
      "Macro f1 = 0.9427011828295777\n",
      "accuracy = 0.9427692307692308\n",
      "TEST Average LOSS IS: 0.22318666578556695\n",
      "EPOCH 74 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.215881\n",
      "Macro precision = 0.9480581178985708\n",
      "Macro recall = 0.9457396046353101\n",
      "Macro f1 = 0.9457697360237612\n",
      "accuracy = 0.9458461538461539\n",
      "TEST Average LOSS IS: 0.2229965340838343\n",
      "EPOCH 75 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.214284\n",
      "Macro precision = 0.9480581178985708\n",
      "Macro recall = 0.9457396046353101\n",
      "Macro f1 = 0.9457697360237612\n",
      "accuracy = 0.9458461538461539\n",
      "TEST Average LOSS IS: 0.2225153002025298\n",
      "EPOCH 76 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.214264\n",
      "Macro precision = 0.9485993758087843\n",
      "Macro recall = 0.9463568885859275\n",
      "Macro f1 = 0.9463884491045781\n",
      "accuracy = 0.9464615384615385\n",
      "TEST Average LOSS IS: 0.2216579539297656\n",
      "EPOCH 77 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.213660\n",
      "Macro precision = 0.9480581178985708\n",
      "Macro recall = 0.9457396046353101\n",
      "Macro f1 = 0.9457697360237612\n",
      "accuracy = 0.9458461538461539\n",
      "TEST Average LOSS IS: 0.22137949201928722\n",
      "EPOCH 78 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.213777\n",
      "Macro precision = 0.9455600222591858\n",
      "Macro recall = 0.94391047489207\n",
      "Macro f1 = 0.9439425972195529\n",
      "accuracy = 0.944\n",
      "TEST Average LOSS IS: 0.22003763011124283\n",
      "EPOCH 79 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.211606\n",
      "Macro precision = 0.9487554369677255\n",
      "Macro recall = 0.9463531015678255\n",
      "Macro f1 = 0.9463834880484956\n",
      "accuracy = 0.9464615384615385\n",
      "TEST Average LOSS IS: 0.22045794379212538\n",
      "EPOCH 80 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.212579\n",
      "Macro precision = 0.9485993758087843\n",
      "Macro recall = 0.9463568885859275\n",
      "Macro f1 = 0.9463884491045781\n",
      "accuracy = 0.9464615384615385\n",
      "TEST Average LOSS IS: 0.2198057794057885\n",
      "EPOCH 81 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.211125\n",
      "Macro precision = 0.9477575602645217\n",
      "Macro recall = 0.945747178671514\n",
      "Macro f1 = 0.9457794413841636\n",
      "accuracy = 0.9458461538461539\n",
      "TEST Average LOSS IS: 0.21910016160068627\n",
      "EPOCH 82 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.212686\n",
      "Macro precision = 0.9491418099013036\n",
      "Macro recall = 0.9469741725365447\n",
      "Macro f1 = 0.9470070652190401\n",
      "accuracy = 0.947076923076923\n",
      "TEST Average LOSS IS: 0.21876969443645214\n",
      "EPOCH 83 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.209825\n",
      "Macro precision = 0.9492953006849044\n",
      "Macro recall = 0.9469703855184428\n",
      "Macro f1 = 0.9470022420232211\n",
      "accuracy = 0.947076923076923\n",
      "TEST Average LOSS IS: 0.21879259076260985\n",
      "EPOCH 84 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.209808\n",
      "Macro precision = 0.9499928420873083\n",
      "Macro recall = 0.9475838824509581\n",
      "Macro f1 = 0.9476160515416334\n",
      "accuracy = 0.9476923076923077\n",
      "TEST Average LOSS IS: 0.2186711520107972\n",
      "EPOCH 85 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.209799\n",
      "Macro precision = 0.9508571428571428\n",
      "Macro recall = 0.9481935923653715\n",
      "Macro f1 = 0.9482248520710059\n",
      "accuracy = 0.9483076923076923\n",
      "TEST Average LOSS IS: 0.21939565006473513\n",
      "EPOCH 86 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.210015\n",
      "Macro precision = 0.9499928420873083\n",
      "Macro recall = 0.9475838824509581\n",
      "Macro f1 = 0.9476160515416334\n",
      "accuracy = 0.9476923076923077\n",
      "TEST Average LOSS IS: 0.21819268747453702\n",
      "EPOCH 87 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.210344\n",
      "Macro precision = 0.9484488856692377\n",
      "Macro recall = 0.9463606756040294\n",
      "Macro f1 = 0.946393246629742\n",
      "accuracy = 0.9464615384615385\n",
      "TEST Average LOSS IS: 0.21688389428118976\n",
      "EPOCH 88 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.207862\n",
      "Macro precision = 0.9499928420873083\n",
      "Macro recall = 0.9475838824509581\n",
      "Macro f1 = 0.9476160515416334\n",
      "accuracy = 0.9476923076923077\n",
      "TEST Average LOSS IS: 0.2177501266164549\n",
      "EPOCH 89 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.207019\n",
      "Macro precision = 0.9496854245171031\n",
      "Macro recall = 0.947591456487162\n",
      "Macro f1 = 0.947625585787679\n",
      "accuracy = 0.9476923076923077\n",
      "TEST Average LOSS IS: 0.21616687165188153\n",
      "EPOCH 90 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.207741\n",
      "Macro precision = 0.950532483471238\n",
      "Macro recall = 0.9482011664015754\n",
      "Macro f1 = 0.9482347480226812\n",
      "accuracy = 0.9483076923076923\n",
      "TEST Average LOSS IS: 0.21664145108176272\n",
      "EPOCH 91 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.209019\n",
      "Macro precision = 0.9513928034931305\n",
      "Macro recall = 0.9488108763159888\n",
      "Macro f1 = 0.9488437259745672\n",
      "accuracy = 0.9489230769230769\n",
      "TEST Average LOSS IS: 0.21720172026226045\n",
      "EPOCH 92 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.207476\n",
      "Macro precision = 0.9503785595557748\n",
      "Macro recall = 0.9482049534196774\n",
      "Macro f1 = 0.9482394590511554\n",
      "accuracy = 0.9483076923076923\n",
      "TEST Average LOSS IS: 0.21541310776782216\n",
      "EPOCH 93 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.207160\n",
      "Macro precision = 0.9512302472068911\n",
      "Macro recall = 0.9488146633340907\n",
      "Macro f1 = 0.9488486150347715\n",
      "accuracy = 0.9489230769230769\n",
      "TEST Average LOSS IS: 0.21593302325856015\n",
      "EPOCH 94 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.205812\n",
      "Macro precision = 0.950532483471238\n",
      "Macro recall = 0.9482011664015754\n",
      "Macro f1 = 0.9482347480226812\n",
      "accuracy = 0.9483076923076923\n",
      "TEST Average LOSS IS: 0.21554779619922718\n",
      "EPOCH 95 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.206977\n",
      "Macro precision = 0.9510733044073989\n",
      "Macro recall = 0.9488184503521927\n",
      "Macro f1 = 0.9488533479963217\n",
      "accuracy = 0.9489230769230769\n",
      "TEST Average LOSS IS: 0.21506964936212664\n",
      "EPOCH 96 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.206332\n",
      "Macro precision = 0.9502302240157277\n",
      "Macro recall = 0.9482087404377793\n",
      "Macro f1 = 0.948244012230338\n",
      "accuracy = 0.9483076923076923\n",
      "TEST Average LOSS IS: 0.2145818569214899\n",
      "EPOCH 97 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.205082\n",
      "Macro precision = 0.9519296385659624\n",
      "Macro recall = 0.949428160266606\n",
      "Macro f1 = 0.9494625009860975\n",
      "accuracy = 0.9495384615384616\n",
      "TEST Average LOSS IS: 0.21532679225767626\n",
      "EPOCH 98 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n",
      "Average_loss for EPOCH is: 0.205543\n",
      "Macro precision = 0.952630664834378\n",
      "Macro recall = 0.9500416571991214\n",
      "Macro f1 = 0.950076407276385\n",
      "accuracy = 0.9501538461538461\n",
      "TEST Average LOSS IS: 0.21518308697047028\n",
      "EPOCH 99 end.\n",
      " ======================\n",
      "\n",
      "\n",
      "\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset[dataset.columns[1:]].to_numpy()\n",
    "y = dataset[\"class\"].to_numpy()\n",
    "\n",
    "x_train, x_test, y_train1, y_test1 = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "cls = MLP(n_per_layer=(22,10,5), num_classes=2, lr=0.00003)\n",
    "\n",
    "for i in range(100):\n",
    "    if i%100 == 0:\n",
    "        print(f\"EPOCH {i} start.\")\n",
    "\n",
    "    cls.train(x_train, y_train1, batch_size=100)\n",
    "    \n",
    "    if (i + 1) %1 == 0:\n",
    "        cls.test(x_test, y_test1)\n",
    "        print(f\"EPOCH {i} end.\\n ======================\\n\\n\\n\\n======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.9 0.1]\n",
      "  [0.9 0.1]]\n",
      "\n",
      " [[0.1 0.9]\n",
      "  [0.1 0.9]]\n",
      "\n",
      " [[0.9 0.1]\n",
      "  [0.9 0.1]]]\n",
      "[[[1. 0.]\n",
      "  [0. 1.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.018,  0.018],\n",
       "       [ 0.018, -0.018],\n",
       "       [ 0.162, -0.162]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# def preload_y(y_true):\n",
    "#     otv = np.zeros((y_true.shape[0], 3))\n",
    "#     otv[np.arange(otv.shape[0]), y_true] += 1\n",
    "#     return otv\n",
    "\n",
    "# def categirical_crossentropy( y_pred, y_true):\n",
    "#     y_pred = softmax( y_pred) \n",
    "#     print(y_pred)\n",
    "#     y_true = preload_y(y_true)\n",
    "#     cross_ent = np.sum(y_true * np.log(y_pred), axis=-1) / y_true.shape[0]\n",
    "#     grad = y_pred - y_true\n",
    "#     return cross_ent, grad\n",
    "\n",
    "# # preload_y(np.array([0,0,1,2,0,1,2,1]))\n",
    "\n",
    "# categirical_crossentropy(y_pred=np.array([[0.8,0.1,0.1],\n",
    "#                                           [0.1,0.8,0.1],\n",
    "#                                           [0.1,0.1,0.8],\n",
    "#                                           [0.8,0.1,0.1],\n",
    "#                                           [0.8,0.1,0.1],\n",
    "#                                           [0.8,0.1,0.1]]),\n",
    "                        # y_true=np.array([0,1,2,0,0,1]))\n",
    "# y  = np.array([0,1,2,0,0,1])\n",
    "y_otv = np.array([[1,0,0,0], [0,1,0,0], [0,1,0,0]])\n",
    "y = np.array([[0.9, 0.05, 0.03, 0.02], [0.05, 0.9, 0.03, 0.02], [0.05, 0.03, 0.9, 0.02]])\n",
    "y_otv = np.array([[1,0], [0,1], [0,1]])\n",
    "y = np.array([[0.9, 0.1], [0.1, 0.9], [0.9, 0.1]])\n",
    "# y = np.array([[0.9,0.5,0.5]])\n",
    "# yal = np.diagflat(y)\n",
    "# # np.einsum('ij,ik->ijk', y, y)\n",
    "# # yal-np.einsum('ij,ik->ijk', y, y)\n",
    "# grad_logits = y.copy()\n",
    "# grad_logits[range(X.shape[0]), y_otv] -= 1\n",
    "# grad_logits\n",
    "\n",
    "# class Softmax:\n",
    "#     def backward(out_grad):\n",
    "\n",
    "#         if i == k:\n",
    "#             dx_dy = y[i](1 - y[i])\n",
    "#         else:\n",
    "#             dx_dy = -y[i]*y[k]\n",
    "\n",
    "fard = y - y_otv\n",
    "\n",
    "n = y.shape[1]\n",
    "y_y_matr = np.tile(y, (n, 1))\n",
    "y_new = np.tile(y[:, :, np.newaxis], (1, 1, n))\n",
    "# print(y)\n",
    "# print(np.identity(n)[np.newaxis,:])\n",
    "# print(y_new)\n",
    "trans_axes = (0,2, 1)\n",
    "np.transpose(y_new, axes = trans_axes)\n",
    "print(np.transpose(y_new, axes = trans_axes))\n",
    "print(np.identity(n)[np.newaxis,:])\n",
    "# print(np.transpose(y_new, axes = (0,2, 1)))\n",
    "\n",
    "# print(y_new)\n",
    "# print(np.transpose(fard[:, np.newaxis], axes = trans_axes))\n",
    "np.matmul((np.identity(n)[np.newaxis,:]  - np.transpose(y_new, axes = trans_axes)) * y_new, np.transpose(fard[:, np.newaxis], axes = trans_axes)).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "# print(np.transpose(fard[:, np.newaxis], axes = trans_axes))\n",
    "\n",
    "# np.dot((np.identity(n)  - y_y_matr) * y_y_matr.T,  fard.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
