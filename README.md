# Easy_ML_and_NN
3-rd year Neurotechnology projects. Here we will present simple implementations of popular machine learning algorithms in Python without using Scikit-learn, PyTorch and TensorFlow libraries


You can view the current working version in the *_clean file, and the development process in the others



# Лабораторная работа №2  
**Векторные представления текста и косинусное сходство**

---

## 1. Теоретические сведения

Векторные представления слов и текстов являются базовым инструментом в задачах обработки естественного языка. Они позволяют переводить текстовые данные в числовое пространство фиксированной размерности, в котором возможно применение методов машинного обучения и математических метрик сходства.

Одним из классических подходов к обучению векторных представлений слов является модель **GloVe (Global Vectors for Word Representation)**. В основе GloVe лежит идея использования глобальной статистики совместных встреч слов в корпусе. Для каждой пары слов \( (i, j) \) вычисляется количество их совместных появлений \( X_{ij} \) в фиксированном окне контекста. Модель обучается так, чтобы скалярное произведение векторов слов аппроксимировало логарифм этого значения:

\[
w_i^\top \tilde{w}_j + b_i + \tilde{b}_j \approx \log X_{ij}
\]

В результате обучения каждому слову сопоставляется плотный вектор фиксированной размерности, отражающий его семантические свойства.

Для сравнения векторов в задачах семантического анализа широко используется **косинусное сходство**, определяемое как:

\[
\text{cosine\_similarity}(a, b) = \frac{a \cdot b}{\|a\|\|b\|}
\]

Данная мера учитывает угол между векторами и не зависит от их длины, что делает её удобной для работы с эмбеддингами.

---

## 2. Используемые данные

В качестве исходных данных использовалась тестовая выборка, разбитая на 4 папки по количеству классов предложений.  
Каждая папка содержит файлы формата `.tsv`, соответствующие отдельным документам.

Формат `.tsv` файлов:

```

original_word    stem    lemma

````

Каждая строка соответствует одному токену. Название файла (без расширения) используется как идентификатор документа.

---

## 3. Реализация векторизации текста

### 3.1 Сегментация текста

Для сегментации текста на предложения и токены использовалась библиотека **SpaCy**.  
Алгоритм сегментации:

1. Из `.tsv` файла формируется строка текста (на основе оригинальных слов).
2. SpaCy выполняет:
   - разбиение текста на предложения;
   - токенизацию предложений.

### 3.2 Векторизация токенов

Для каждого токена определяется его лемма и выполняется поиск соответствующего индекса в словаре `word2id`, полученном при обучении модели GloVe.  
Если токен отсутствует в словаре, он пропускается.

Вектор токена извлекается из матрицы эмбеддингов `word_embeddings`.

---

## 4. Векторизация предложений и документов

### 4.1 Вектор предложения

Для каждого предложения вычисляется средний вектор по всем входящим в него токенам:

\[
v_{sent} = \frac{1}{N} \sum_{k=1}^{N} v_k
\]

где \( v_k \) — вектор \( k \)-го токена предложения.

### 4.2 Вектор документа

Вектор документа вычисляется как среднее значение векторов всех предложений документа:

\[
v_{doc} = \frac{1}{M} \sum_{s=1}^{M} v_{sent}^{(s)}
\]

Таким образом, каждый документ представляется одним вектором фиксированной размерности, совпадающей с размерностью эмбеддингов слов.

---

## 5. Косинусное сходство

Для сравнения векторных представлений слов и документов использовалось косинусное сходство.  
В реализации на PyTorch нормализация выполняется по L2-норме, после чего сходство вычисляется как скалярное произведение нормализованных векторов:

```python
(vectors_i / torch.norm(vectors_i, dim=1, keepdim=True)) @
(vectors_j / torch.norm(vectors_j, dim=1, keepdim=True)).T
````

Данный подход позволяет эффективно находить наиболее близкие по смыслу слова или документы.

---

## 6. Векторизация тестовой выборки

Для всей тестовой выборки был выполнен следующий алгоритм:

1. Итерация по всем `.tsv` файлам во всех папках.
2. Формирование текста документа.
3. Векторизация документа описанным выше методом.
4. Сохранение результатов в файл формата `.tsv`.

Формат выходного файла:

```
doc_id<TAB>v1<TAB>v2<TAB>...<TAB>vD
```

где:

* `doc_id` — идентификатор документа (имя файла без расширения);
* `v1 ... vD` — компоненты векторного представления документа;
* размерность векторов одинакова для всех документов;
* количество строк соответствует количеству документов в тестовой выборке.

---

## 7. Результаты

В результате выполнения лабораторной работы:

* реализован метод векторизации произвольного текста на основе GloVe-эмбеддингов;
* выполнена сегментация текста на предложения и токены с использованием SpaCy;
* получены векторные представления предложений и документов;
* выполнена векторизация всей тестовой выборки;
* результаты сохранены в формате `.tsv`, пригодном для дальнейшего использования в задачах классификации и анализа текста.

Полученные векторные представления могут быть использованы в качестве входных признаков для моделей машинного обучения, а также для анализа семантического сходства между документами.
